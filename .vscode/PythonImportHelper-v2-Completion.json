[
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "html",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "html",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "html",
        "importPath": "lxml",
        "description": "lxml",
        "isExtraImport": true,
        "detail": "lxml",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "HTMLParser",
        "importPath": "selectolax.parser",
        "description": "selectolax.parser",
        "isExtraImport": true,
        "detail": "selectolax.parser",
        "documentation": {}
    },
    {
        "label": "HTMLParser",
        "importPath": "selectolax.parser",
        "description": "selectolax.parser",
        "isExtraImport": true,
        "detail": "selectolax.parser",
        "documentation": {}
    },
    {
        "label": "HTMLParser",
        "importPath": "selectolax.parser",
        "description": "selectolax.parser",
        "isExtraImport": true,
        "detail": "selectolax.parser",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "urlencode",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "get_text",
        "kind": 2,
        "importPath": "src.main",
        "description": "src.main",
        "peekOfCode": "def get_text(html,selector,index=0):\n  parser = HTMLParser(html)\n  return parser.css(selector)[index].text(deep=False).strip()\n# def build_url(base_url, path, query_dict):\n#     # Returns a list in the structure of urlparse.ParseResult\n#     url_parts = list(urllib.parse.urlparse(base_url))\n#     url_parts[2] = path\n#     url_parts[4] = urllib.parse.urlencode(args_dict)\n#     return urllib.parse.urlunparse(url_parts)\n# titles=[]",
        "detail": "src.main",
        "documentation": {}
    },
    {
        "label": "build_url",
        "kind": 2,
        "importPath": "src.main",
        "description": "src.main",
        "peekOfCode": "def build_url(base_url,path='',query_dict={}):\n  url = f\"{base_url}/{path}{urlencode(query_dict)}\"\n  return url\n# https://www.youtube.com/results?/search_query=as7ab+lkahf\n# https://www.youtube.com/results?search_query=as7ab+lkahf\n# url = 'https://www.indeed.co.in/jobs?q=' + skill + \\\n#   '&l=' + place + '&start=' + str(page * 10)\npage = 1\nurl = build_url('https://ma.indeed.com','jobs?',{'q': 'dÃ©veloppeur','l':'Rabat','sort':'date','start':str(page * 10) if page > 1 else ''})\nprint('url: ', url)",
        "detail": "src.main",
        "documentation": {}
    },
    {
        "label": "html_content",
        "kind": 5,
        "importPath": "src.main",
        "description": "src.main",
        "peekOfCode": "html_content = '''\n<a\n  href=\"https://www.bezorgdekrant.nl/en/jobs/depot-holder-in-enkhuizen\"\n  class=\"vacancy-card icon-chevron-right\"\n  data-id=\"274476\"\n  data-category=\"Depot Holder\"\n  data-lat=\"52.7153849\"\n  data-long=\"5.2846831\"\n>\n  <h4>",
        "detail": "src.main",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "src.main",
        "description": "src.main",
        "peekOfCode": "parser = HTMLParser(html_content)\ndef get_text(html,selector,index=0):\n  parser = HTMLParser(html)\n  return parser.css(selector)[index].text(deep=False).strip()\n# def build_url(base_url, path, query_dict):\n#     # Returns a list in the structure of urlparse.ParseResult\n#     url_parts = list(urllib.parse.urlparse(base_url))\n#     url_parts[2] = path\n#     url_parts[4] = urllib.parse.urlencode(args_dict)\n#     return urllib.parse.urlunparse(url_parts)",
        "detail": "src.main",
        "documentation": {}
    },
    {
        "label": "page",
        "kind": 5,
        "importPath": "src.main",
        "description": "src.main",
        "peekOfCode": "page = 1\nurl = build_url('https://ma.indeed.com','jobs?',{'q': 'dÃ©veloppeur','l':'Rabat','sort':'date','start':str(page * 10) if page > 1 else ''})\nprint('url: ', url)\n# https://ma.indeed.com/jobs?q=d%C3%A9veloppeur&l=Rabat\n# https://ma.indeed.com/jobs?q=d%C3%A9veloppeur&l=Rabat\n# res = requests.get(url)\n# print(res.text)",
        "detail": "src.main",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "src.main",
        "description": "src.main",
        "peekOfCode": "url = build_url('https://ma.indeed.com','jobs?',{'q': 'dÃ©veloppeur','l':'Rabat','sort':'date','start':str(page * 10) if page > 1 else ''})\nprint('url: ', url)\n# https://ma.indeed.com/jobs?q=d%C3%A9veloppeur&l=Rabat\n# https://ma.indeed.com/jobs?q=d%C3%A9veloppeur&l=Rabat\n# res = requests.get(url)\n# print(res.text)",
        "detail": "src.main",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "url = \"https://www.bezorgdekrant.nl/en/all-jobs\"\nresponse = requests.get(url)\ncontent = response.text\n# print('content: ')\n# print(content)\n# BeautifulSoup\nstart_time_bs = time.time()\nsoup = BeautifulSoup(content,'lxml')\njob_titles_bs = [el.contents[2].get_text().strip() for el in soup.select('a.vacancy-card > h4')]\nprint(\"ðŸš€ length:\", len(job_titles_bs))",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "response = requests.get(url)\ncontent = response.text\n# print('content: ')\n# print(content)\n# BeautifulSoup\nstart_time_bs = time.time()\nsoup = BeautifulSoup(content,'lxml')\njob_titles_bs = [el.contents[2].get_text().strip() for el in soup.select('a.vacancy-card > h4')]\nprint(\"ðŸš€ length:\", len(job_titles_bs))\nend_time_bs = time.time()",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "content",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "content = response.text\n# print('content: ')\n# print(content)\n# BeautifulSoup\nstart_time_bs = time.time()\nsoup = BeautifulSoup(content,'lxml')\njob_titles_bs = [el.contents[2].get_text().strip() for el in soup.select('a.vacancy-card > h4')]\nprint(\"ðŸš€ length:\", len(job_titles_bs))\nend_time_bs = time.time()\nbs_time = end_time_bs - start_time_bs",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "start_time_bs",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "start_time_bs = time.time()\nsoup = BeautifulSoup(content,'lxml')\njob_titles_bs = [el.contents[2].get_text().strip() for el in soup.select('a.vacancy-card > h4')]\nprint(\"ðŸš€ length:\", len(job_titles_bs))\nend_time_bs = time.time()\nbs_time = end_time_bs - start_time_bs\n# ---------------------------\n# Selectolax\nstart_time_selectolax = time.time()\nparser = HTMLParser(content)",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "soup",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "soup = BeautifulSoup(content,'lxml')\njob_titles_bs = [el.contents[2].get_text().strip() for el in soup.select('a.vacancy-card > h4')]\nprint(\"ðŸš€ length:\", len(job_titles_bs))\nend_time_bs = time.time()\nbs_time = end_time_bs - start_time_bs\n# ---------------------------\n# Selectolax\nstart_time_selectolax = time.time()\nparser = HTMLParser(content)\njob_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "job_titles_bs",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "job_titles_bs = [el.contents[2].get_text().strip() for el in soup.select('a.vacancy-card > h4')]\nprint(\"ðŸš€ length:\", len(job_titles_bs))\nend_time_bs = time.time()\nbs_time = end_time_bs - start_time_bs\n# ---------------------------\n# Selectolax\nstart_time_selectolax = time.time()\nparser = HTMLParser(content)\njob_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]\njob_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "end_time_bs",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "end_time_bs = time.time()\nbs_time = end_time_bs - start_time_bs\n# ---------------------------\n# Selectolax\nstart_time_selectolax = time.time()\nparser = HTMLParser(content)\njob_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]\njob_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]\nprint(\"ðŸš€ length:\", len(job_titles_selectolax))\nend_time_selectolax = time.time()",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "bs_time",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "bs_time = end_time_bs - start_time_bs\n# ---------------------------\n# Selectolax\nstart_time_selectolax = time.time()\nparser = HTMLParser(content)\njob_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]\njob_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]\nprint(\"ðŸš€ length:\", len(job_titles_selectolax))\nend_time_selectolax = time.time()\nselectolax_time = end_time_selectolax - start_time_selectolax",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "start_time_selectolax",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "start_time_selectolax = time.time()\nparser = HTMLParser(content)\njob_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]\njob_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]\nprint(\"ðŸš€ length:\", len(job_titles_selectolax))\nend_time_selectolax = time.time()\nselectolax_time = end_time_selectolax - start_time_selectolax\nh4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "parser = HTMLParser(content)\njob_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]\njob_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]\nprint(\"ðŸš€ length:\", len(job_titles_selectolax))\nend_time_selectolax = time.time()\nselectolax_time = end_time_selectolax - start_time_selectolax\nh4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "job_titles_selectolax",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "job_titles_selectolax = [node.text().strip() for node in parser.css('#jobList > a:nth-child(1) > h4')]\njob_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]\nprint(\"ðŸš€ length:\", len(job_titles_selectolax))\nend_time_selectolax = time.time()\nselectolax_time = end_time_selectolax - start_time_selectolax\nh4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts\nfor text in h4_texts:",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "job_titles_selectolax",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "job_titles_selectolax = [el.text().strip() for el in parser.css('a.vacancy-card > h4:nth-of-type(2)')]\nprint(\"ðŸš€ length:\", len(job_titles_selectolax))\nend_time_selectolax = time.time()\nselectolax_time = end_time_selectolax - start_time_selectolax\nh4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts\nfor text in h4_texts:\n    print(text)",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "end_time_selectolax",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "end_time_selectolax = time.time()\nselectolax_time = end_time_selectolax - start_time_selectolax\nh4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts\nfor text in h4_texts:\n    print(text)\n# \n# lxml",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "selectolax_time",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "selectolax_time = end_time_selectolax - start_time_selectolax\nh4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts\nfor text in h4_texts:\n    print(text)\n# \n# lxml\n# start_time_lxml = time.time()",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "h4_elements",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "h4_elements = parser.css('h4')\n# Extract text content from each <h4> element\nh4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts\nfor text in h4_texts:\n    print(text)\n# \n# lxml\n# start_time_lxml = time.time()\n# tree = html.fromstring(content)",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "h4_texts",
        "kind": 5,
        "importPath": "src.main_",
        "description": "src.main_",
        "peekOfCode": "h4_texts = [element.text().strip() for element in h4_elements]\n# Print the extracted texts\nfor text in h4_texts:\n    print(text)\n# \n# lxml\n# start_time_lxml = time.time()\n# tree = html.fromstring(content)\n# job_titles_lxml = tree.xpath('//h3[@class=\"card-title\"]/text()')\n# end_time_lxml = time.time()",
        "detail": "src.main_",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "headers = {\n    \"User-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n# Skills and Place of Work\nskill = input('Enter your Skill: ').strip()\nplace = input('Enter the location: ').strip()\nno_of_pages = int(input('Enter the #pages to scrape: '))\n# Creating the Main Directory\nmain_dir = os.getcwd() + '\\\\'\nif not os.path.exists(main_dir):\n    os.mkdir(main_dir)",
        "detail": "src.scrape_indeed",
        "documentation": {}
    },
    {
        "label": "skill",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "skill = input('Enter your Skill: ').strip()\nplace = input('Enter the location: ').strip()\nno_of_pages = int(input('Enter the #pages to scrape: '))\n# Creating the Main Directory\nmain_dir = os.getcwd() + '\\\\'\nif not os.path.exists(main_dir):\n    os.mkdir(main_dir)\n    print('Base Directory Created Successfully.')\n# Name of the CSV File\nfile_name = skill.title() + '_' + place.title() + '_Jobs.csv'",
        "detail": "src.scrape_indeed",
        "documentation": {}
    },
    {
        "label": "place",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "place = input('Enter the location: ').strip()\nno_of_pages = int(input('Enter the #pages to scrape: '))\n# Creating the Main Directory\nmain_dir = os.getcwd() + '\\\\'\nif not os.path.exists(main_dir):\n    os.mkdir(main_dir)\n    print('Base Directory Created Successfully.')\n# Name of the CSV File\nfile_name = skill.title() + '_' + place.title() + '_Jobs.csv'\n# Path of the CSV File",
        "detail": "src.scrape_indeed",
        "documentation": {}
    },
    {
        "label": "no_of_pages",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "no_of_pages = int(input('Enter the #pages to scrape: '))\n# Creating the Main Directory\nmain_dir = os.getcwd() + '\\\\'\nif not os.path.exists(main_dir):\n    os.mkdir(main_dir)\n    print('Base Directory Created Successfully.')\n# Name of the CSV File\nfile_name = skill.title() + '_' + place.title() + '_Jobs.csv'\n# Path of the CSV File\nfile_path = main_dir + file_name",
        "detail": "src.scrape_indeed",
        "documentation": {}
    },
    {
        "label": "main_dir",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "main_dir = os.getcwd() + '\\\\'\nif not os.path.exists(main_dir):\n    os.mkdir(main_dir)\n    print('Base Directory Created Successfully.')\n# Name of the CSV File\nfile_name = skill.title() + '_' + place.title() + '_Jobs.csv'\n# Path of the CSV File\nfile_path = main_dir + file_name\n# Writing to the CSV File\nwith open(file_path, mode='w') as file:",
        "detail": "src.scrape_indeed",
        "documentation": {}
    },
    {
        "label": "file_name",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "file_name = skill.title() + '_' + place.title() + '_Jobs.csv'\n# Path of the CSV File\nfile_path = main_dir + file_name\n# Writing to the CSV File\nwith open(file_path, mode='w') as file:\n    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n    # Adding the Column Names to the CSV File\n    writer.writerow(\n        ['JOB_NAME', 'COMPANY', 'LOCATION', 'POSTED', 'APPLY_LINK'])\n    # Requesting and getting the webpage using requests",
        "detail": "src.scrape_indeed",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "src.scrape_indeed",
        "description": "src.scrape_indeed",
        "peekOfCode": "file_path = main_dir + file_name\n# Writing to the CSV File\nwith open(file_path, mode='w') as file:\n    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n    # Adding the Column Names to the CSV File\n    writer.writerow(\n        ['JOB_NAME', 'COMPANY', 'LOCATION', 'POSTED', 'APPLY_LINK'])\n    # Requesting and getting the webpage using requests\n    print(f'\\nScraping in progress...\\n')\n    for page in range(no_of_pages):",
        "detail": "src.scrape_indeed",
        "documentation": {}
    }
]